{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicuai/Book-SD-MasterGuide/blob/main/ComfyJapan_LTX2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHeV1M2PQdoX"
      },
      "source": [
        "# [ComfyJapan] ComfyUI LTX-2 å‹•ç”»ç”Ÿæˆ\n",
        "\n",
        "## ğŸš€ æ¦‚è¦\n",
        "\n",
        "Lightricksç¤¾ã®æœ€æ–°å‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ« **LTX-2** ã‚’ Google Colab ã§é‹ç”¨ã™ã‚‹ãŸã‚ã®çµ±åˆç’°å¢ƒã§ã™ã€‚ãƒãƒ¼ã‚ºåˆ¶å¾¡ï¼ˆPose to Videoï¼‰ ã¨ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆText to Videoï¼‰ ã®ä¸¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«å®Œå…¨å¯¾å¿œã—ã€24æ™‚é–“ç¨¼åƒã‚’æƒ³å®šã—ãŸã€Œè‡ªå·±ä¿®å¾©æ©Ÿèƒ½ã€ã‚’å‚™ãˆã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "### ğŸ› ï¸ æŠ€è¡“è§£èª¬ã¨æ¨å¥¨ç’°å¢ƒ\n",
        "\n",
        "* **æ¨å¥¨GPU**: **L4 GPU (22.5GB)** ä»¥ä¸Šã€‚ç”Ÿæˆé€Ÿåº¦ã¨å®‰å®šæ€§ã®é¢ã‹ã‚‰ **A100 / H100** ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚\n",
        "* **ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**: L4 GPUã§ã¯VRAMå®¹é‡ãŒãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆ19B + 12Bï¼‰ã«å¯¾ã—ã¦ã‚·ãƒ“ã‚¢ãªãŸã‚ã€è‡ªå‹•çš„ã«ãƒ¡ãƒ¢ãƒªç¯€ç´„ãƒ¢ãƒ¼ãƒ‰ï¼ˆ`--lowvram`ï¼‰ã‚’é©ç”¨ã—ã¾ã™ã€‚\n",
        "* **è‡ªå·±ä¿®å¾©ãƒ­ã‚¸ãƒƒã‚¯**:\n",
        "* **ãƒ¢ãƒ‡ãƒ«é…ç½®**: Gemma 3IT ç­‰ã®è¤‡é›‘ãªéšå±¤ãƒ¢ãƒ‡ãƒ«ã‚’ ComfyUI èªè­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸è‡ªå‹•æ•´ç†ã—ã¾ã™ã€‚\n",
        "* **è‡ªå‹•å†èµ·å‹•**: ã‚µãƒ¼ãƒãƒ¼ãŒãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆOOMï¼‰ãªã©ã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãŸå ´åˆã€ã‚¹ãƒ¬ãƒƒãƒ‰ãŒãã‚Œã‚’æ¤œçŸ¥ã—ã¦ Discord ã«å ±å‘Šã—ã€è‡ªå‹•çš„ã«ãƒ—ãƒ­ã‚»ã‚¹ã‚’å†ç«‹ã¡ä¸Šã’ã—ã¾ã™ã€‚\n",
        "\n",
        "\n",
        "\n",
        "### ğŸ“‹ äº‹å‰æº–å‚™\n",
        "\n",
        "1. **HuggingFace**: [LTX-2 Repository](https://huggingface.co/Lightricks/LTX-2) ã§åˆ©ç”¨è¨±è«¾ã‚’æ¸ˆã¾ã›ãŸã‚¢ã‚«ã‚¦ãƒ³ãƒˆã® `HF_TOKEN` ã‚’ã€Colabã®ã€ŒğŸ”‘ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚\n",
        "2. **Discord**: é€šçŸ¥ç”¨ Webhook URL ã‚’ `DISCORD_URL` ã¨ã—ã¦è¨­å®šã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrj4C9pxNS_i"
      },
      "source": [
        "# ğŸ¥šå„ç¨®ãƒ¢ãƒ‡ãƒ«ã®DL ï¼† ComfyUIã®èµ·å‹•ğŸ£\n",
        "##ï¼œã¯ã˜ã‚ã‚‹å‰ã®æº–å‚™ï¼\n",
        "### â¶ Google Colab ã®ã€Œã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€ã« `HF_TOKEN` ã¨ `DISCORD_URL` ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚\n",
        "### â·ï¼ˆâ–¶ï¼‰ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦èµ·å‹•ã€‚å®Œäº†ã™ã‚‹ã¨ Discord ã«é€šçŸ¥ãŒå±Šãã¾ã™ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jp2-pOzeK-GF"
      },
      "outputs": [],
      "source": [
        "import os, socket, time, threading, requests, shutil, subprocess, re, sys\n",
        "from collections import deque\n",
        "from google.colab import userdata, output\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”‘ è¨­å®šç¢ºèª & Discordé€šçŸ¥ï¼ˆãƒ†ã‚¹ãƒˆæ©Ÿèƒ½ä»˜ï¼‰\n",
        "# ==========================================\n",
        "def get_secret(key):\n",
        "    try: return userdata.get(key)\n",
        "    except: return None\n",
        "\n",
        "hf_token = get_secret('HF_TOKEN')\n",
        "if hf_token: os.environ['HF_TOKEN'] = hf_token\n",
        "webhook_url = get_secret('DISCORD_URL')\n",
        "\n",
        "def send_discord(msg):\n",
        "    if webhook_url:\n",
        "        try:\n",
        "            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®šã—ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®é€ä¿¡æ¼ã‚Œã‚’é˜²ã\n",
        "            requests.post(webhook_url, json={\"content\": msg}, timeout=15)\n",
        "        except Exception as e:\n",
        "            print(f\"ğŸ“¡ Discordé€ä¿¡å¤±æ•—: {e}\")\n",
        "\n",
        "# GPUã®æ­£ä½“ã‚’æš´ã\n",
        "print(\"ğŸ–¥ï¸ ç‰©ç†GPUã®æ­£ç¢ºãªæƒ…å ±ã‚’ç¢ºèªä¸­...\")\n",
        "try:\n",
        "    gpu_info = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"]).decode().strip()\n",
        "except:\n",
        "    gpu_info = \"Unknown GPU\"\n",
        "\n",
        "# ğŸš€ ãƒ†ã‚¹ãƒˆé€šçŸ¥ã‚’é€ä¿¡\n",
        "send_discord(f\"ğŸ”” **é€šçŸ¥ãƒ†ã‚¹ãƒˆé€ä¿¡**\\nğŸ’» æ¤œå‡ºGPU: `{gpu_info}`\\nğŸ› ï¸ ã‚µãƒ¼ãƒãƒ¼æ§‹ç¯‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚\")\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ“ ç’°å¢ƒæ§‹ç¯‰ (DWPose ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾ç­–è¾¼)\n",
        "# ==========================================\n",
        "%cd /content\n",
        "clear_output()\n",
        "print(f\"ğŸ› ï¸ {gpu_info} ç”¨ã«ç’°å¢ƒã‚’æ§‹ç¯‰ä¸­...\")\n",
        "\n",
        "# ä¾å­˜é–¢ä¿‚ï¼ˆç«¶åˆã‚’æŠ‘åˆ¶ã—ã¤ã¤ DWPose åŠ é€Ÿç”¨ã® onnxruntime-gpu ã‚’è¿½åŠ ï¼‰\n",
        "!pip install -q \"protobuf<5.0.0\" \"huggingface_hub[cli]<1.0\"\n",
        "!pip install -q torch==2.8.0 torchvision==0.23.0 xformers==0.0.32.post1 triton==3.4 requests spandrel onnxruntime-gpu\n",
        "!pip install -q av albumentations onnx opencv-python\n",
        "\n",
        "if not os.path.exists(\"ComfyUI\"):\n",
        "    !git clone https://github.com/Comfy-Org/ComfyUI.git\n",
        "!pip install -q -r /content/ComfyUI/requirements.txt\n",
        "\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "def clone_node(url):\n",
        "    name = url.split(\"/\")[-1].replace(\".git\", \"\")\n",
        "    if not os.path.exists(name): subprocess.run([\"git\", \"clone\", url], check=False)\n",
        "\n",
        "# æœ€æ–°ã® Comfy-Org æº–æ‹ \n",
        "clone_node(\"https://github.com/Comfy-Org/ComfyUI-Manager\")\n",
        "clone_node(\"https://github.com/Lightricks/ComfyUI-LTXVideo\")\n",
        "clone_node(\"https://github.com/Isi-dev/comfyui_controlnet_aux\")\n",
        "clone_node(\"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite\")\n",
        "clone_node(\"https://github.com/kijai/ComfyUI-KJNodes.git\")\n",
        "clone_node(\"https://github.com/rgthree/rgthree-comfy.git\")\n",
        "\n",
        "# å„ãƒãƒ¼ãƒ‰ã®ä¾å­˜é–¢ä¿‚ã‚’è§£æ±º\n",
        "for root, dirs, files in os.walk(\"/content/ComfyUI/custom_nodes\"):\n",
        "    if \"requirements.txt\" in files:\n",
        "        subprocess.run([\"pip\", \"install\", \"-q\", \"-r\", os.path.join(root, \"requirements.txt\")], check=False)\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "# ========================================\n",
        "# ğŸ“¥ ãƒ¢ãƒ‡ãƒ«æº–å‚™ (æœ€æ–°æ¨å¥¨ hf download æ–¹å¼)\n",
        "# ========================================\n",
        "print(\"ğŸ“¥ LTX-2 é–¢é€£ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "\n",
        "# 1. Gemma 3 IT é…ç½®ä¿®æ­£\n",
        "!hf download Comfy-Org/ltx-2 split_files/text_encoders/gemma_3_12B_it.safetensors --local-dir models/text_encoders --quiet\n",
        "g_path = \"models/text_encoders/split_files/text_encoders/gemma_3_12B_it.safetensors\"\n",
        "if os.path.exists(g_path): shutil.move(g_path, \"models/text_encoders/gemma_3_12B_it.safetensors\")\n",
        "\n",
        "# 2. ãƒ¢ãƒ‡ãƒ«ä¸€æ‹¬å–å¾—\n",
        "models_to_get = [\n",
        "    (\"Lightricks/LTX-2\", \"ltx-2-19b-dev-fp8.safetensors\", \"models/checkpoints\"),\n",
        "    (\"Lightricks/LTX-2\", \"ltx-2-19b-distilled.safetensors\", \"models/checkpoints\"),\n",
        "    (\"Lightricks/LTX-2-19b-IC-LoRA-Pose-Control\", \"ltx-2-19b-ic-lora-pose-control.safetensors\", \"models/loras\"),\n",
        "    (\"Lightricks/LTX-2-19b-IC-LoRA-Depth-Control\", \"ltx-2-19b-ic-lora-depth-control.safetensors\", \"models/loras\"),\n",
        "    (\"Comfy-Org/lotus\", \"lotus-depth-d-v1-1.safetensors\", \"models/diffusion_models\"),\n",
        "    (\"stabilityai/sd-vae-ft-mse-original\", \"vae-ft-mse-840000-ema-pruned.safetensors\", \"models/vae\"),\n",
        "    (\"Lightricks/LTX-2\", \"ltx-2-spatial-upscaler-x2-1.0.safetensors\", \"models/latent_upscale_models\")\n",
        "]\n",
        "for repo, file, folder in models_to_get:\n",
        "    if not os.path.exists(os.path.join(folder, file)):\n",
        "        subprocess.run([\"hf\", \"download\", repo, file, \"--local-dir\", folder, \"--quiet\"])\n",
        "\n",
        "# ã‚¢ã‚»ãƒƒãƒˆå–å¾— (aria2cã«ã‚ˆã‚‹é«˜é€ŸåŒ–)\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -d input -o minimalist_sky_view_lounge.png https://huggingface.co/Lightricks/LTX-2/resolve/main/minimalist_sky_view_lounge.png --quiet\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -d input -o minimalist_sky_view_lounge.mp4 https://huggingface.co/Lightricks/LTX-2/resolve/main/minimalist_sky_view_lounge.mp4 --quiet\n",
        "\n",
        "# ========================================\n",
        "# ğŸš€ ã‚µãƒ¼ãƒ“ã‚¹ç›£è¦– & ã‚¯ãƒ©ãƒƒã‚·ãƒ¥è§£æ\n",
        "# ========================================\n",
        "restart_count = 0\n",
        "notified_ready = False\n",
        "\n",
        "def status_monitor(port):\n",
        "    global notified_ready\n",
        "    while True:\n",
        "        try:\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            sock.settimeout(2)\n",
        "            res = sock.connect_ex(('127.0.0.1', port))\n",
        "            sock.close()\n",
        "            if res == 0 and not notified_ready:\n",
        "                proxy_url = output.eval_js(f'google.colab.kernel.proxyPort({port})')\n",
        "                status = \"ğŸ‰ ComfyUI æº–å‚™å®Œäº†ï¼\" if restart_count == 0 else \"ğŸ”„ è‡ªå‹•å†èµ·å‹•ã«æˆåŠŸ\"\n",
        "                msg = f\"ğŸš€ **{status}**\\nğŸ’» å®Ÿç¨¼åƒGPU: `{gpu_info}`\\nğŸ”— URL: {proxy_url}\"\n",
        "                send_discord(msg)\n",
        "                print(f\"\\n{msg}\\n\")\n",
        "                notified_ready = True\n",
        "            if res != 0: notified_ready = False\n",
        "        except: pass\n",
        "        time.sleep(10)\n",
        "\n",
        "threading.Thread(target=status_monitor, daemon=True, args=(8188,)).start()\n",
        "\n",
        "# èµ·å‹•ãƒ•ãƒ©ã‚°åˆ¤å®š (H100/A100ã¯VRAMã‚’ãƒ•ãƒ«æ´»ç”¨)\n",
        "mem_flags = \"\" if any(x in gpu_info for x in [\"H100\", \"A100\"]) else \"--lowvram\"\n",
        "\n",
        "print(f\"ğŸš€ {gpu_info} ãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ä¸­...\")\n",
        "try:\n",
        "    while True:\n",
        "        # Popen ã§ãƒ­ã‚°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ã‚­ãƒ£ãƒ³\n",
        "        process = subprocess.Popen(\n",
        "            f\"python main.py {mem_flags} --cache-none --dont-print-server\",\n",
        "            shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        "        )\n",
        "\n",
        "        log_window = deque(maxlen=20) # ç›´è¿‘20è¡Œã‚’ä¿æŒ\n",
        "        for line in process.stdout:\n",
        "            print(line, end=\"\")\n",
        "            log_window.append(line.strip())\n",
        "\n",
        "            # ã‚¤ãƒ™ãƒ³ãƒˆæ¤œçŸ¥\n",
        "            if \"got prompt\" in line.lower():\n",
        "                send_discord(f\"ğŸ“¥ **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ç†** (GPU: `{gpu_info}`)\")\n",
        "            exec_match = re.search(r\"Prompt executed in (\\d+\\.\\d+) seconds\", line)\n",
        "            if exec_match:\n",
        "                send_discord(f\"âœ… **ç”Ÿæˆå®Œäº†** (æ‰€è¦æ™‚é–“: {exec_match.group(1)}ç§’)\")\n",
        "\n",
        "        process.wait()\n",
        "        if process.returncode != 0:\n",
        "            restart_count += 1\n",
        "            # ã‚¯ãƒ©ãƒƒã‚·ãƒ¥åŸå› ã®è¨ºæ–­\n",
        "            logs_str = \"\\n\".join(log_window)\n",
        "            reason = \"ä¸æ˜ãªã‚¨ãƒ©ãƒ¼\"\n",
        "            if \"OutOfMemoryError\" in logs_str: reason = \"GPUãƒ¡ãƒ¢ãƒªä¸è¶³ (OOM)\"\n",
        "            elif \"Killed\" in logs_str: reason = \"ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å¼·åˆ¶çµ‚äº† (RAMä¸è¶³ãªã©)\"\n",
        "\n",
        "            report = f\"âš ï¸ **ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ**\\nğŸ”´ æ¨å®šåŸå› : `{reason}`\\nğŸ”„ {restart_count}å›ç›®ã®å†èµ·å‹•ã‚’è¡Œã„ã¾ã™ã€‚\\nğŸ“ ç›´è¿‘ã®ãƒ­ã‚°æœ«å°¾:\\n```{log_window[-1]}```\"\n",
        "            send_discord(report)\n",
        "            time.sleep(5)\n",
        "        else: break\n",
        "except KeyboardInterrupt:\n",
        "    print(\"â¹ï¸ åœæ­¢ã•ã‚Œã¾ã—ãŸã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVgoPYQfcQMf"
      },
      "outputs": [],
      "source": [
        "# @markdown # ğŸ‘ˆ çµ±åˆèµ·å‹•ãƒœã‚¿ãƒ³ï¼ˆH100/A100 è¨ºæ–­ãƒ»é€šçŸ¥å®Œå…¨ç‰ˆï¼‰\n",
        "# @markdown ---\n",
        "# @markdown â€» èµ·å‹•ç›´å¾Œã«ã€Œãƒ†ã‚¹ãƒˆé€šçŸ¥ã€ãŒDiscordã«å±Šãã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "# @markdown â€» ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ™‚ã¯ã€ãƒ­ã‚°ã‹ã‚‰ã€ŒOOMï¼ˆãƒ¡ãƒ¢ãƒªä¸è¶³ï¼‰ã€ç­‰ã‚’åˆ¤å®šã—ã¦è©³ç´°å ±å‘Šã—ã¾ã™ã€‚\n",
        "\n",
        "import os, socket, time, threading, requests, shutil, subprocess, re, sys\n",
        "from collections import deque\n",
        "from google.colab import userdata, output\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ”‘ è¨­å®šç¢ºèª & Discordãƒ†ã‚¹ãƒˆ\n",
        "# ==========================================\n",
        "def get_secret(key):\n",
        "    try: return userdata.get(key)\n",
        "    except: return None\n",
        "\n",
        "hf_token = get_secret('HF_TOKEN')\n",
        "if hf_token: os.environ['HF_TOKEN'] = hf_token\n",
        "webhook_url = get_secret('DISCORD_URL')\n",
        "\n",
        "def send_discord(msg):\n",
        "    if webhook_url:\n",
        "        try:\n",
        "            requests.post(webhook_url, json={\"content\": msg}, timeout=10)\n",
        "        except Exception as e:\n",
        "            print(f\"ğŸ“¡ Discordé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "\n",
        "# GPUã®æ­£ä½“ã‚’æš´ã\n",
        "print(\"ğŸ–¥ï¸ ç¾åœ¨ã®ç‰©ç†GPUã‚’ç¢ºèªã—ã¾ã™...\")\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
        "gpu_info = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"]).decode().strip()\n",
        "send_discord(f\"ğŸš€ **ComfyUI èµ·å‹•ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹**\\nğŸ’» ç‰©ç†GPU: `{gpu_info}`\")\n",
        "\n",
        "# ==========================================\n",
        "# ğŸ“ ç’°å¢ƒæ§‹ç¯‰ (DWPoseåŠ é€Ÿãƒ‘ãƒ¼ãƒ„è¿½åŠ )\n",
        "# ==========================================\n",
        "%cd /content\n",
        "clear_output()\n",
        "print(f\"ğŸ› ï¸ {gpu_info} ç”¨ã«ç’°å¢ƒã‚’æ§‹ç¯‰ä¸­ (ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾ç­–æ¸ˆ)...\")\n",
        "\n",
        "# ä¾å­˜é–¢ä¿‚ï¼ˆç«¶åˆæŠ‘åˆ¶ + åŠ é€Ÿç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰\n",
        "!pip install -q \"huggingface_hub[cli]<1.0\" \"protobuf<5.0.0\"\n",
        "!pip install -q torch==2.8.0 torchvision==0.23.0 xformers==0.0.32.post1 triton==3.4 requests spandrel onnxruntime-gpu\n",
        "!pip install -q av albumentations onnx opencv-python\n",
        "\n",
        "if not os.path.exists(\"ComfyUI\"):\n",
        "    !git clone https://github.com/Comfy-Org/ComfyUI.git\n",
        "!pip install -q -r /content/ComfyUI/requirements.txt\n",
        "\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "def clone_node(url):\n",
        "    name = url.split(\"/\")[-1].replace(\".git\", \"\")\n",
        "    if not os.path.exists(name): subprocess.run([\"git\", \"clone\", url], check=False)\n",
        "\n",
        "clone_node(\"https://github.com/Comfy-Org/ComfyUI-Manager\")\n",
        "clone_node(\"https://github.com/Lightricks/ComfyUI-LTXVideo\")\n",
        "clone_node(\"https://github.com/Isi-dev/comfyui_controlnet_aux\")\n",
        "clone_node(\"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite\")\n",
        "clone_node(\"https://github.com/kijai/ComfyUI-KJNodes.git\")\n",
        "clone_node(\"https://github.com/rgthree/rgthree-comfy.git\")\n",
        "\n",
        "# ãƒãƒ¼ãƒ‰ã®ä¾å­˜é–¢ä¿‚è§£æ±º\n",
        "for root, dirs, files in os.walk(\"/content/ComfyUI/custom_nodes\"):\n",
        "    if \"requirements.txt\" in files:\n",
        "        subprocess.run([\"pip\", \"install\", \"-q\", \"-r\", os.path.join(root, \"requirements.txt\")], check=False)\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "# ========================================\n",
        "# ğŸ“¥ ãƒ¢ãƒ‡ãƒ«æº–å‚™ (æœ€æ–° hf download æ–¹å¼)\n",
        "# ========================================\n",
        "print(\"ğŸ“¥ LTX-2 é–¢é€£ãƒ¢ãƒ‡ãƒ«ã‚’æœ€æ–°æ–¹å¼ã§æº–å‚™ä¸­...\")\n",
        "\n",
        "# Gemma 3 IT é…ç½®ä¿®æ­£ (Text Encoder)\n",
        "!hf download Comfy-Org/ltx-2 split_files/text_encoders/gemma_3_12B_it.safetensors --local-dir models/text_encoders --quiet\n",
        "g_wrong = \"models/text_encoders/split_files/text_encoders/gemma_3_12B_it.safetensors\"\n",
        "if os.path.exists(g_wrong): shutil.move(g_wrong, \"models/text_encoders/gemma_3_12B_it.safetensors\")\n",
        "\n",
        "# ä¸€æ‹¬ãƒ¢ãƒ‡ãƒ«å–å¾—\n",
        "models = [\n",
        "    (\"Lightricks/LTX-2\", \"ltx-2-19b-dev-fp8.safetensors\", \"models/checkpoints\"),\n",
        "    (\"Lightricks/LTX-2\", \"ltx-2-19b-distilled.safetensors\", \"models/checkpoints\"), # è’¸ç•™ç‰ˆ\n",
        "    (\"Lightricks/LTX-2-19b-IC-LoRA-Pose-Control\", \"ltx-2-19b-ic-lora-pose-control.safetensors\", \"models/loras\"),\n",
        "    (\"Lightricks/LTX-2-19b-IC-LoRA-Depth-Control\", \"ltx-2-19b-ic-lora-depth-control.safetensors\", \"models/loras\"),\n",
        "    (\"Comfy-Org/lotus\", \"lotus-depth-d-v1-1.safetensors\", \"models/diffusion_models\"),\n",
        "    (\"stabilityai/sd-vae-ft-mse-original\", \"vae-ft-mse-840000-ema-pruned.safetensors\", \"models/vae\"),\n",
        "    (\"Lightricks/LTX-2\", \"ltx-2-spatial-upscaler-x2-1.0.safetensors\", \"models/latent_upscale_models\")\n",
        "]\n",
        "for repo, file, folder in models:\n",
        "    if not os.path.exists(os.path.join(folder, file)):\n",
        "        subprocess.run([\"hf\", \"download\", repo, file, \"--local-dir\", folder, \"--quiet\"])\n",
        "\n",
        "# ã‚¢ã‚»ãƒƒãƒˆ\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -d input -o minimalist_sky_view_lounge.png https://huggingface.co/Lightricks/LTX-2/resolve/main/minimalist_sky_view_lounge.png --quiet\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -d input -o minimalist_sky_view_lounge.mp4 https://huggingface.co/Lightricks/LTX-2/resolve/main/minimalist_sky_view_lounge.mp4 --quiet\n",
        "\n",
        "# ========================================\n",
        "# ğŸš€ ã‚µãƒ¼ãƒãƒ¼ç›£è¦– & ã‚¯ãƒ©ãƒƒã‚·ãƒ¥è§£æ\n",
        "# ========================================\n",
        "restart_count = 0\n",
        "notified_ready = False\n",
        "\n",
        "def status_monitor(port):\n",
        "    global notified_ready\n",
        "    while True:\n",
        "        try:\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            res = sock.connect_ex(('127.0.0.1', port))\n",
        "            sock.close()\n",
        "            if res == 0 and not notified_ready:\n",
        "                proxy_url = output.eval_js(f'google.colab.kernel.proxyPort({port})')\n",
        "                msg = f\"ğŸ‰ **ComfyUI æº–å‚™å®Œäº†ï¼**\\nğŸ’» GPU: `{gpu_info}`\\nğŸ”— URL: {proxy_url}\"\n",
        "                send_discord(msg)\n",
        "                print(f\"\\n{msg}\\n\")\n",
        "                notified_ready = True\n",
        "            if res != 0: notified_ready = False\n",
        "        except: pass\n",
        "        time.sleep(10)\n",
        "\n",
        "threading.Thread(target=status_monitor, daemon=True, args=(8188,)).start()\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— (ã‚¯ãƒ©ãƒƒã‚·ãƒ¥è§£æä»˜)\n",
        "mem_flags = \"\" if any(x in gpu_info for x in [\"H100\", \"A100\"]) else \"--lowvram\"\n",
        "print(f\"ğŸš€ {gpu_info} é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã§èµ·å‹•ä¸­...\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        process = subprocess.Popen(\n",
        "            f\"python main.py {mem_flags} --cache-none --dont-print-server\",\n",
        "            shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        "        )\n",
        "\n",
        "        log_window = deque(maxlen=15) # ç›´è¿‘15è¡Œã®ãƒ­ã‚°ã‚’ä¿æŒ\n",
        "        for line in process.stdout:\n",
        "            print(line, end=\"\")\n",
        "            log_window.append(line.strip())\n",
        "\n",
        "            # æ­£å¸¸ã‚¤ãƒ™ãƒ³ãƒˆ\n",
        "            if \"got prompt\" in line.lower():\n",
        "                send_discord(f\"ğŸ“¥ **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ç†** (GPU: `{gpu_info}`)\")\n",
        "\n",
        "            exec_match = re.search(r\"Prompt executed in (\\d+\\.\\d+) seconds\", line)\n",
        "            if exec_match:\n",
        "                send_discord(f\"âœ… **å‹•ç”»ç”Ÿæˆå®Œäº†ï¼** (æ‰€è¦æ™‚é–“: {exec_match.group(1)}ç§’)\")\n",
        "\n",
        "        process.wait()\n",
        "        if process.returncode != 0:\n",
        "            restart_count += 1\n",
        "            # ã‚¯ãƒ©ãƒƒã‚·ãƒ¥åŸå› ã®æ¨æ¸¬\n",
        "            reason = \"ä¸æ˜ãªã‚¨ãƒ©ãƒ¼\"\n",
        "            logs_str = \"\\n\".join(log_window)\n",
        "            if \"OutOfMemoryError\" in logs_str: reason = \"GPUãƒ¡ãƒ¢ãƒªä¸è¶³ (OOM)\"\n",
        "            elif \"Segmentation fault\" in logs_str: reason = \"ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é•å (ãƒ¡ãƒ¢ãƒªä¸æ•´åˆ)\"\n",
        "            elif \"Killed\" in logs_str: reason = \"ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å¼·åˆ¶çµ‚äº† (RAMä¸è¶³ç­‰)\"\n",
        "\n",
        "            error_report = f\"âš ï¸ **ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ**\\nğŸ”´ åŸå› : `{reason}`\\nğŸ”„ {restart_count}å›ç›®ã®å†èµ·å‹•ã‚’è¡Œã„ã¾ã™ã€‚\\nğŸ“ ç›´è¿‘ã®ãƒ­ã‚°:\\n```{log_window[-1]}```\"\n",
        "            send_discord(error_report)\n",
        "            time.sleep(5)\n",
        "        else: break\n",
        "except KeyboardInterrupt:\n",
        "    print(\"â¹ï¸ åœæ­¢ã•ã‚Œã¾ã—ãŸã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeqjdDwGdOH4"
      },
      "source": [
        "# æŠ€è¡“çš„ãªæ”¹å–„ç‚¹ã¾ã¨ã‚\n",
        "## æŠ€è¡“è§£èª¬ï¼š2026å¹´ç‰ˆ LTX-2 é‹ç”¨ã‚¬ã‚¤ãƒ‰\n",
        "\n",
        "ã“ã®ç’°å¢ƒã§ã¯ã€ä»¥ä¸‹ã®é«˜åº¦ãªæœ€é©åŒ–ãŒæ–½ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "VRAM ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ: A100 80GB ã§ã¯ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ VRAM ã«ä¿æŒã™ã‚‹é«˜é€Ÿæ¨è«–ã‚’ã€L4 22.5GB ã§ã¯ --lowvram ã‚’ç”¨ã„ãŸå …å®Ÿãªå‹•ä½œã‚’è‡ªå‹•çš„ã«é¸æŠã—ã¾ã™ã€‚\n",
        "\n",
        "ã‚¢ã‚»ãƒƒãƒˆä¸æ•´åˆã®è§£æ¶ˆ: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒè¦æ±‚ã™ã‚‹ woman_with_paper ç³»ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã€ç¾åœ¨ã®æ­£ã—ã„é…å¸ƒURLã‹ã‚‰è‡ªå‹•å–å¾—ã—ã¾ã™ã€‚\n",
        "\n",
        "ãƒãƒ«ãƒãƒªãƒã‚¸ãƒˆãƒªãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: å˜ä¸€ã®é…å¸ƒå…ƒã§ã¯æƒã‚ãªã„ Pose Control LoRA ã‚„ Text Encoder ã‚’ã€è¤‡æ•°ã® Hugging Face ãƒªãƒã‚¸ãƒˆãƒªã‚’æ¨ªæ–­ã—ã¦ä¸€æ‹¬ã§æƒãˆã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã§ã€æº–å‚™ã¯ä¸‡å…¨ã§ã™ã€‚æä¾›ã•ã‚ŒãŸ JSON ãƒ•ã‚¡ã‚¤ãƒ« ã‚’èª­ã¿è¾¼ã¿ã€LTX-2 ã®é©šç•°çš„ãªå‹•ç”»ç”Ÿæˆèƒ½åŠ›ã‚’ã”ä½“é¨“ãã ã•ã„ï¼\n",
        "# è¬è¾ï¼šã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ AICU Japan ãŒãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã—ã¦ãŠã‚Šã¾ã™ã€‚è³ªå•ã¯ [X@AICUai](https://x.com/AICUai) ã¾ã§ã€‚"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}